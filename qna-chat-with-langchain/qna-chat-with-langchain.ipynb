{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI Service - Chat on private data using LangChain\n",
    "\n",
    "Firstly, create a file called `.env` in this folder, and add the following content, obviously with your values:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxxxx\n",
    "OPENAI_API_BASE=https://xxxxxxx.openai.azure.com/\n",
    "```\n",
    "\n",
    "Then, let's install all dependencies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing requirements\n",
    "\n",
    "You can install the requirements from here, but a better option is to select to install the requirements when creating a virtual enviroment (.venv), see the main readme file to learn how to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Load environment variables (set OPENAI_API_KEY and OPENAI_API_BASE in .env)\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Azure OpenAI Service API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Init LLM and embeddings model\n",
    "llm = AzureChatOpenAI(deployment_name=\"gpt4-32\", temperature=0, openai_api_version=\"2023-03-15-preview\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", chunk_size=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load up our documents from the `data` directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below throws an error due to codec issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import DirectoryLoader\n",
    "# from langchain.document_loaders import TextLoader\n",
    "# from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# loader = DirectoryLoader('../data/qna/', glob=\"*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "# documents = loader.load()\n",
    "# text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I have replaced it with the code below\n",
    "\n",
    "I got help from GPT4 rewriting the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required classes from the langchain library  \n",
    "from langchain.document_loaders import DirectoryLoader  \n",
    "from langchain.document_loaders import TextLoader  \n",
    "from langchain.text_splitter import TokenTextSplitter  \n",
    "  \n",
    "# Define a custom document class that includes 'page_content' and 'metadata' attributes  \n",
    "class CustomDocument:  \n",
    "    def __init__(self, page_content, metadata=None):  \n",
    "        self.page_content = page_content  # Store the text content of the document  \n",
    "        self.metadata = metadata if metadata is not None else {}  # Store the metadata, defaulting to an empty dictionary if not provided  \n",
    "  \n",
    "# Define a custom text loader class that inherits from the langchain TextLoader class  \n",
    "class CustomTextLoader(TextLoader):  \n",
    "    def __init__(self, file_path, encoding='utf-8', errors='strict'):  \n",
    "        super().__init__(file_path, encoding)  # Call the base class constructor with the file path and encoding  \n",
    "        self.errors = errors  # Store the error handling strategy for decoding the text file  \n",
    "  \n",
    "    def load(self):  \n",
    "        # Try to load the text file using the specified encoding and error handling strategy  \n",
    "        try:  \n",
    "            with open(self.file_path, encoding=self.encoding, errors=self.errors) as f:  \n",
    "                text = f.read()  \n",
    "        except UnicodeDecodeError as e:  \n",
    "            # If there's an issue with decoding the text file, raise a runtime error with a helpful message  \n",
    "            raise RuntimeError(f\"Could not load text file '{self.file_path}'\") from e  \n",
    "        # Return a list containing a single CustomDocument object with the loaded text  \n",
    "        return [CustomDocument(page_content=text)]  \n",
    "  \n",
    "# Initialize the DirectoryLoader with the custom text loader class and the desired encoding and error handling strategy  \n",
    "loader = DirectoryLoader('../data/qna/', glob=\"*.txt\", loader_cls=lambda file_path: CustomTextLoader(file_path, encoding='utf-8', errors='replace'))  \n",
    "  \n",
    "# Load the documents from the specified directory using the custom text loader  \n",
    "documents = loader.load()  \n",
    "  \n",
    "# Initialize the TokenTextSplitter with the desired chunk size and overlap  \n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)  \n",
    "  \n",
    "# Split the loaded documents into smaller chunks using the TokenTextSplitter  \n",
    "docs = text_splitter.split_documents(documents)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's ingest them into FAISS so we can efficiently query our embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents=docs, embedding=embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a chain that can do the whole chat on our embedding database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Adapt if needed\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                           retriever=db.as_retriever(),\n",
    "                                           condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "                                           return_source_documents=True,\n",
    "                                           verbose=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI service provides access to powerful language models, including GPT-3, Codex, and Embeddings series, through REST APIs, Python SDK, or the web-based interface in the Azure OpenAI Studio. These models can be adapted for various tasks such as content generation, summarization, semantic search, and natural language to code translation. The service offers features like virtual network support, managed identity via Azure Active Directory, and responsible AI content filtering. Azure OpenAI ensures compatibility and a smooth transition from OpenAI APIs while providing the security and enterprise capabilities of Microsoft Azure.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"what is azure openai service?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to easy implement chat conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is Azure OpenAI Service?\n",
      "Answer: Azure OpenAI Service is a platform that provides access to OpenAI's powerful language models, including the GPT-3, Codex, and Embeddings model series, through REST APIs, Python SDK, or the web-based interface in the Azure OpenAI Studio. These models can be adapted for various tasks such as content generation, summarization, semantic search, and natural language to code translation. Azure OpenAI Service offers features like virtual network support, managed identity via Azure Active Directory, and responsible AI content filtering. It is designed to provide advanced language AI capabilities with the security and enterprise promise of Microsoft Azure.\n",
      "Question: Which regions does the service support?\n",
      "Answer: The Azure OpenAI Service is supported in the following regions:\n",
      "\n",
      "1. East US\n",
      "2. South Central US\n",
      "3. West Europe\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "query = \"what is Azure OpenAI Service?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", result[\"answer\"])\n",
    "\n",
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"Which regions does the service support?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(\"Question:\", query)\n",
    "print(\"Answer:\", result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-qna-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4ee1bbf3137c7ea9420c4fd488a55642063e5739fe2a7286130d9ba47405b69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
